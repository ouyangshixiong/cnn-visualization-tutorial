#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
PyTorchçŒ«ç‹—åˆ†ç±»å™¨ + å®Œæ•´å¯è§†åŒ–
åœ¨venv_linuxè™šæ‹Ÿç¯å¢ƒä¸­è¿è¡Œ

ä½¿ç”¨è¯´æ˜ï¼š
1. å…ˆæ¿€æ´»è™šæ‹Ÿç¯å¢ƒï¼šsource venv_linux/bin/activate
2. è¿è¡Œï¼špython examples/pytorch/cat_dog_classifier.py
"""

import os
import sys
import warnings

# ç¡®ä¿åœ¨è™šæ‹Ÿç¯å¢ƒä¸­è¿è¡Œ
if not sys.prefix.endswith('venv_linux'):
    warnings.warn("å»ºè®®åœ¨venv_linuxè™šæ‹Ÿç¯å¢ƒä¸­è¿è¡Œæ­¤è„šæœ¬")

# æ·»åŠ é¡¹ç›®è·¯å¾„åˆ°Pythonè·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.dirname(__file__)))
sys.path.insert(0, project_root)

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Dataset
import torchvision.models as models
import matplotlib.pyplot as plt
import numpy as np
from PIL import Image
import cv2
from pathlib import Path

# å¯¼å…¥å¯è§†åŒ–å·¥å…·
from visualization.pytorch.layer_visualizer import CNNLayerVisualizer
from visualization.pytorch.cam_visualizer import (
    GradCAM, GradCAMPlusPlus, ScoreCAM, compare_cam_methods
)

# é…ç½®matplotlibæ”¯æŒä¸­æ–‡
#plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS']
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False


class SimpleCNN(nn.Module):
    """ç®€å•çš„CNNæ¨¡å‹ç”¨äºçŒ«ç‹—åˆ†ç±»"""
    
    def __init__(self, num_classes=2):
        super(SimpleCNN, self).__init__()
        
        # å·ç§¯å±‚
        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm2d(32)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm2d(64)
        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        self.bn3 = nn.BatchNorm2d(128)
        
        # æ± åŒ–å±‚
        self.pool = nn.MaxPool2d(2, 2)
        self.dropout = nn.Dropout(0.5)
        
        # å…¨è¿æ¥å±‚
        self.fc1 = nn.Linear(128 * 28 * 28, 512)
        self.fc2 = nn.Linear(512, 256)
        self.fc3 = nn.Linear(256, num_classes)
        
        # æ¿€æ´»å‡½æ•°
        self.relu = nn.ReLU()
    
    def forward(self, x):
        # ç¬¬ä¸€å±‚ï¼šå·ç§¯ + æ‰¹å½’ä¸€åŒ– + æ¿€æ´» + æ± åŒ–
        x = self.pool(self.relu(self.bn1(self.conv1(x))))  # 224 -> 112
        
        # ç¬¬äºŒå±‚
        x = self.pool(self.relu(self.bn2(self.conv2(x))))  # 112 -> 56
        
        # ç¬¬ä¸‰å±‚
        x = self.pool(self.relu(self.bn3(self.conv3(x))))  # 56 -> 28
        
        # å±•å¹³
        x = x.view(x.size(0), -1)
        
        # å…¨è¿æ¥å±‚
        x = self.dropout(self.relu(self.fc1(x)))
        x = self.dropout(self.relu(self.fc2(x)))
        x = self.fc3(x)
        
        return x


class CatDogDataset(Dataset):
    """çŒ«ç‹—æ•°æ®é›† - ä½¿ç”¨è™šæ‹Ÿæ•°æ®æ¼”ç¤º"""
    
    def __init__(self, root_dir, transform=None, train=True, num_samples=100):
        self.root_dir = Path(root_dir)
        self.transform = transform
        self.train = train
        self.num_samples = num_samples
        
        # åˆ›å»ºæ•°æ®é›†ç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        self.root_dir.mkdir(exist_ok=True)
        (self.root_dir / 'cats').mkdir(exist_ok=True)
        (self.root_dir / 'dogs').mkdir(exist_ok=True)
        
        # ç”Ÿæˆè™šæ‹Ÿå›¾ç‰‡æ•°æ®
        self.images = []
        self.labels = []
        self._generate_virtual_data()
    
    def _generate_virtual_data(self):
        """ç”Ÿæˆè™šæ‹Ÿçš„çŒ«ç‹—å›¾ç‰‡æ•°æ®"""
        for i in range(self.num_samples):
            if i % 2 == 0:
                # çŒ«å›¾ç‰‡ç‰¹å¾ï¼šåæš–è‰²è°ƒï¼Œåœ†å½¢ç‰¹å¾
                img = self._create_cat_image()
                label = 0  # çŒ«
            else:
                # ç‹—å›¾ç‰‡ç‰¹å¾ï¼šåå†·è‰²è°ƒï¼Œé•¿å½¢ç‰¹å¾
                img = self._create_dog_image()
                label = 1  # ç‹—
            
            self.images.append(img)
            self.labels.append(label)
    
    def _create_cat_image(self):
        """åˆ›å»ºæ¨¡æ‹Ÿçš„çŒ«å›¾ç‰‡"""
        # åˆ›å»ºåŸºç¡€å›¾åƒ
        img = np.random.randint(100, 200, (224, 224, 3), dtype=np.uint8)
        
        # æ·»åŠ çŒ«è„¸ç‰¹å¾ï¼ˆåœ†å½¢åŒºåŸŸï¼‰
        center = (np.random.randint(80, 140), np.random.randint(80, 140))
        radius = np.random.randint(30, 50)
        
        y, x = np.ogrid[:224, :224]
        mask = (x - center[0]) ** 2 + (y - center[1]) ** 2 <= radius ** 2
        
        # æš–è‰²è°ƒ
        img[mask] = [255, 200, 150]  # æ©™è‰²
        
        # æ·»åŠ è€³æœµ
        ear1 = (center[0] - radius//2, center[1] - radius)
        ear2 = (center[0] + radius//2, center[1] - radius)
        
        for ear in [ear1, ear2]:
            ear_mask = (x - ear[0]) ** 2 + (y - ear[1]) ** 2 <= (radius//3) ** 2
            img[ear_mask] = [200, 150, 100]  # æ£•è‰²
        
        return img
    
    def _create_dog_image(self):
        """åˆ›å»ºæ¨¡æ‹Ÿçš„ç‹—å›¾ç‰‡"""
        # åˆ›å»ºåŸºç¡€å›¾åƒ
        img = np.random.randint(50, 150, (224, 224, 3), dtype=np.uint8)
        
        # æ·»åŠ ç‹—è„¸ç‰¹å¾ï¼ˆæ¤­åœ†å½¢åŒºåŸŸï¼‰
        center = (np.random.randint(80, 140), np.random.randint(80, 140))
        a, b = np.random.randint(40, 60), np.random.randint(30, 45)
        
        y, x = np.ogrid[:224, :224]
        mask = ((x - center[0]) / a) ** 2 + ((y - center[1]) / b) ** 2 <= 1
        
        # å†·è‰²è°ƒ
        img[mask] = [150, 200, 255]  # è“è‰²
        
        # æ·»åŠ è€³æœµï¼ˆé•¿å½¢ï¼‰
        ear_y = center[1] - b
        ear_height = b//2
        ear_width = a//3
        
        ear_mask = (
            (abs(x - center[0]) <= ear_width) & 
            (abs(y - ear_y) <= ear_height)
        )
        img[ear_mask] = [100, 150, 200]  # æ·±è“è‰²
        
        return img
    
    def __len__(self):
        return len(self.images)
    
    def __getitem__(self, idx):
        img = self.images[idx]
        label = self.labels[idx]
        
        # è½¬æ¢ä¸ºPIL Image
        img = Image.fromarray(img)
        
        if self.transform:
            img = self.transform(img)
        else:
            img = transforms.ToTensor()(img)
        
        return img, label


def create_transforms():
    """åˆ›å»ºæ•°æ®è½¬æ¢"""
    train_transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.RandomRotation(degrees=10),
        transforms.ColorJitter(brightness=0.2, contrast=0.2),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                           std=[0.229, 0.224, 0.225])
    ])
    
    test_transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                           std=[0.229, 0.224, 0.225])
    ])
    
    return train_transform, test_transform


def train_model(model, train_loader, val_loader, num_epochs=10, device='cpu'):
    """è®­ç»ƒæ¨¡å‹"""
    model = model.to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)
    
    train_losses = []
    val_losses = []
    train_accuracies = []
    val_accuracies = []
    
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    print(f"æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters())}")
    
    for epoch in range(num_epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0.0
        train_correct = 0
        train_total = 0
        
        for batch_idx, (images, labels) in enumerate(train_loader):
            images, labels = images.to(device), labels.to(device)
            
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            train_total += labels.size(0)
            train_correct += (predicted == labels).sum().item()
            
            if batch_idx % 10 == 0:
                print(f'Epoch: {epoch+1}/{num_epochs}, '
                      f'Batch: {batch_idx}/{len(train_loader)}, '
                      f'Loss: {loss.item():.4f}')
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0.0
        val_correct = 0
        val_total = 0
        
        with torch.no_grad():
            for images, labels in val_loader:
                images, labels = images.to(device), labels.to(device)
                
                outputs = model(images)
                loss = criterion(outputs, labels)
                
                val_loss += loss.item()
                _, predicted = torch.max(outputs.data, 1)
                val_total += labels.size(0)
                val_correct += (predicted == labels).sum().item()
        
        scheduler.step()
        
        # è®¡ç®—æŒ‡æ ‡
        train_losses.append(train_loss / len(train_loader))
        val_losses.append(val_loss / len(val_loader))
        train_accuracies.append(100 * train_correct / train_total)
        val_accuracies.append(100 * val_correct / val_total)
        
        print(f'Epoch [{epoch+1}/{num_epochs}] '
              f'Train Loss: {train_losses[-1]:.4f}, '
              f'Train Acc: {train_accuracies[-1]:.2f}%, '
              f'Val Loss: {val_losses[-1]:.4f}, '
              f'Val Acc: {val_accuracies[-1]:.2f}%')
    
    return train_losses, val_losses, train_accuracies, val_accuracies


def plot_training_history(train_losses, val_losses, train_accuracies, val_accuracies):
    """ç»˜åˆ¶è®­ç»ƒå†å²"""
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
    
    # æŸå¤±æ›²çº¿
    ax1.plot(train_losses, label='è®­ç»ƒæŸå¤±', color='blue')
    ax1.plot(val_losses, label='éªŒè¯æŸå¤±', color='red')
    ax1.set_title('è®­ç»ƒä¸éªŒè¯æŸå¤±')
    ax1.set_xlabel('è½®æ¬¡')
    ax1.set_ylabel('æŸå¤±')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # å‡†ç¡®ç‡æ›²çº¿
    ax2.plot(train_accuracies, label='è®­ç»ƒå‡†ç¡®ç‡', color='blue')
    ax2.plot(val_accuracies, label='éªŒè¯å‡†ç¡®ç‡', color='red')
    ax2.set_title('è®­ç»ƒä¸éªŒè¯å‡†ç¡®ç‡')
    ax2.set_xlabel('è½®æ¬¡')
    ax2.set_ylabel('å‡†ç¡®ç‡ (%)')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # å­¦ä¹ æ›²çº¿å¯¹æ¯”
    ax3.plot(train_losses, label='è®­ç»ƒæŸå¤±', color='blue')
    ax3.plot(val_losses, label='éªŒè¯æŸå¤±', color='red', linestyle='--')
    ax3.set_title('æŸå¤±å¯¹æ¯”')
    ax3.set_xlabel('è½®æ¬¡')
    ax3.set_ylabel('æŸå¤±')
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    
    # å‡†ç¡®ç‡å¯¹æ¯”
    ax4.plot(train_accuracies, label='è®­ç»ƒå‡†ç¡®ç‡', color='blue')
    ax4.plot(val_accuracies, label='éªŒè¯å‡†ç¡®ç‡', color='red', linestyle='--')
    ax4.set_title('å‡†ç¡®ç‡å¯¹æ¯”')
    ax4.set_xlabel('è½®æ¬¡')
    ax4.set_ylabel('å‡†ç¡®ç‡ (%)')
    ax4.legend()
    ax4.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('training_history_pytorch.png', dpi=300, bbox_inches='tight')
    plt.show()


def visualize_cnn_layers(model, sample_image, device='cpu'):
    """å¯è§†åŒ–CNNå„å±‚"""
    visualizer = CNNLayerVisualizer(model)
    
    # è·å–æ‰€æœ‰å·ç§¯å±‚åç§°
    layer_names = visualizer.get_layer_names()
    print(f"å¯ç”¨å±‚: {layer_names}")
    
    # å‡†å¤‡è¾“å…¥
    sample_image = sample_image.unsqueeze(0).to(device)
    
    # å¯è§†åŒ–æ»¤æ³¢å™¨
    print("ğŸ” å¯è§†åŒ–å·ç§¯æ»¤æ³¢å™¨...")
    for layer_name in ['conv1', 'conv2', 'conv3']:
        if layer_name in layer_names:
            fig = visualizer.visualize_conv_filters(layer_name, max_filters=8)
            plt.savefig(f'filters_{layer_name}.png', dpi=300, bbox_inches='tight')
            plt.show()
    
    # å¯è§†åŒ–ç‰¹å¾å›¾
    print("ğŸ” å¯è§†åŒ–ç‰¹å¾å›¾...")
    model.eval()
    with torch.no_grad():
        for layer_name in ['conv1', 'conv2', 'conv3']:
            if layer_name in layer_names:
                fig = visualizer.visualize_feature_maps(sample_image, layer_name, max_maps=8)
                plt.savefig(f'features_{layer_name}.png', dpi=300, bbox_inches='tight')
                plt.show()


def apply_grad_cam(model, sample_image, target_class=0, device='cpu'):
    """åº”ç”¨Grad-CAM"""
    # æ‰¾åˆ°æœ€åä¸€ä¸ªå·ç§¯å±‚
    last_conv_layer = None
    conv_layer_names = []
    
    for name, layer in model.named_modules():
        if isinstance(layer, nn.Conv2d):
            last_conv_layer = layer
            conv_layer_names.append(name)
    
    if last_conv_layer is None:
        print("âŒ æœªæ‰¾åˆ°å·ç§¯å±‚")
        return
    
    print(f"ğŸ“ æ‰¾åˆ°å·ç§¯å±‚: {conv_layer_names}")
    print(f"ğŸ“ ä½¿ç”¨æœ€åä¸€å±‚: {conv_layer_names[-1] if conv_layer_names else 'unknown'}")
    
    # åº”ç”¨ä¸åŒçš„CAMæ–¹æ³•
    sample_image = sample_image.unsqueeze(0).to(device)
    
    # æ¯”è¾ƒä¸åŒçš„CAMæ–¹æ³•
    fig = compare_cam_methods(model, last_conv_layer, sample_image, target_class=target_class)
    plt.savefig('cam_comparison.png', dpi=300, bbox_inches='tight')
    plt.show()


def main():
    """ä¸»å‡½æ•° - åœ¨è™šæ‹Ÿç¯å¢ƒä¸­è¿è¡Œ"""
    print("ğŸ¯ CNNå¯è§†åŒ–æ•™å­¦é¡¹ç›® - PyTorchç‰ˆæœ¬")
    print("=" * 50)
    
    # æ£€æŸ¥è™šæ‹Ÿç¯å¢ƒ
    import sys
    venv_path = sys.prefix
    if 'venv_linux' not in venv_path:
        print("âš ï¸  è­¦å‘Šï¼šå½“å‰ä¸åœ¨venv_linuxè™šæ‹Ÿç¯å¢ƒä¸­")
        print("è¯·è¿è¡Œï¼šsource venv_linux/bin/activate")
    else:
        print(f"âœ… è™šæ‹Ÿç¯å¢ƒå·²æ¿€æ´»: {venv_path}")
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ’» ä½¿ç”¨è®¾å¤‡: {device}")
    
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(42)
    np.random.seed(42)
    
    # åˆ›å»ºæ•°æ®é›†
    print("ğŸ“Š åˆ›å»ºæ•°æ®é›†...")
    train_transform, test_transform = create_transforms()
    
    train_dataset = CatDogDataset('./datasets', train_transform, train=True, num_samples=200)
    test_dataset = CatDogDataset('./datasets', test_transform, train=False, num_samples=50)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=0)
    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=0)
    
    print(f"ğŸ“Š è®­ç»ƒæ ·æœ¬: {len(train_dataset)}, æµ‹è¯•æ ·æœ¬: {len(test_dataset)}")
    
    # åˆ›å»ºæ¨¡å‹
    model = SimpleCNN(num_classes=2)
    print("ğŸ§  æ¨¡å‹ç»“æ„:")
    print(model)
    
    # è®­ç»ƒæ¨¡å‹
    print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
    train_losses, val_losses, train_accuracies, val_accuracies = train_model(
        model, train_loader, test_loader, num_epochs=5, device=device
    )
    
    # ç»˜åˆ¶è®­ç»ƒå†å²
    print("ğŸ“ˆ ç»˜åˆ¶è®­ç»ƒå†å²...")
    plot_training_history(train_losses, val_losses, train_accuracies, val_accuracies)
    
    # ä¿å­˜æ¨¡å‹
    model_path = 'cat_dog_classifier_pytorch.pth'
    torch.save({
        'model_state_dict': model.state_dict(),
        'train_losses': train_losses,
        'val_losses': val_losses,
        'train_accuracies': train_accuracies,
        'val_accuracies': val_accuracies
    }, model_path)
    print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {model_path}")
    
    # å¯è§†åŒ–ç¤ºä¾‹
    print("ğŸ¨ å¼€å§‹å¯è§†åŒ–...")
    sample_image, sample_label = test_dataset[0]
    sample_image = sample_image.to(device)
    
    print(f"ğŸ“„ ç¤ºä¾‹å›¾ç‰‡æ ‡ç­¾: {'çŒ«' if sample_label == 0 else 'ç‹—'}")
    
    # å¯è§†åŒ–CNNå±‚
    visualize_cnn_layers(model, sample_image, device=device)
    
    # åº”ç”¨Grad-CAM
    apply_grad_cam(model, sample_image, target_class=sample_label, device=device)
    
    print("âœ… æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼")
    print("\nğŸ“‹ è¾“å‡ºæ–‡ä»¶:")
    print("- training_history_pytorch.png")
    print("- cat_dog_classifier_pytorch.pth")
    print("- filters_*.png")
    print("- features_*.png")
    print("- cam_comparison.png")


if __name__ == "__main__":
    # æ£€æŸ¥Pythonç‰ˆæœ¬
    import sys
    print(f"Pythonç‰ˆæœ¬: {sys.version}")
    
    # æ£€æŸ¥PyTorchç‰ˆæœ¬
    import torch
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    
    main()